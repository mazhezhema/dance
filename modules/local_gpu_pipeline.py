#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬åœ°GPU Pipelineæ¨¡å—
å¤„ç†Viggleè¾“å‡ºçš„è§†é¢‘ï¼Œè¿›è¡Œè¶…åˆ†è¾¨ç‡ã€æŠ åƒã€èƒŒæ™¯æ›¿æ¢
"""

import os
import cv2
import json
import subprocess
import shutil
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
import logging
from datetime import datetime
import concurrent.futures
import psutil
import GPUtil

logger = logging.getLogger(__name__)

@dataclass
class PipelineJob:
    """GPU Pipelineä»»åŠ¡"""
    input_path: str
    job_id: str
    category: str
    status: str = "pending"  # pending, processing, completed, failed
    stage: str = "superres"  # superres, matting, background, finalize
    
    # è¾“å‡ºè·¯å¾„
    superres_path: Optional[str] = None
    matting_path: Optional[str] = None
    background_path: Optional[str] = None
    final_path: Optional[str] = None
    
    # å¤„ç†ä¿¡æ¯
    start_time: Optional[str] = None
    end_time: Optional[str] = None
    processing_time: Optional[float] = None
    error_message: Optional[str] = None
    
    # è´¨é‡ä¿¡æ¯
    input_resolution: Optional[Tuple[int, int]] = None
    output_resolution: Optional[Tuple[int, int]] = None
    input_size_mb: Optional[float] = None
    output_size_mb: Optional[float] = None

class LocalGPUPipeline:
    def __init__(self, config_path: str = "config/gpu_pipeline_config.json"):
        self.config = self.load_config(config_path)
        self.job_history = []
        self.setup_tools()
        
    def load_config(self, config_path: str) -> dict:
        """åŠ è½½Pipelineé…ç½®"""
        default_config = {
            "tools": {
                "realesrgan_path": "./tools/realesrgan-ncnn-vulkan.exe",
                "rvm_path": "./tools/inference_rvm.py",
                "ffmpeg_path": "ffmpeg"
            },
            "processing": {
                "superres_scale": 2,
                "superres_model": "realesr-animevideov3",
                "max_concurrent_jobs": 2,
                "temp_directory": "./temp_gpu",
                "keep_intermediates": False
            },
            "output": {
                "base_directory": "./final_output",
                "organize_by_category": True,
                "video_codec": "libx264",
                "video_bitrate": "2M",
                "audio_codec": "aac"
            },
            "backgrounds": {
                "library_path": "./backgrounds",
                "default_backgrounds": {
                    "dance": "dance_studio.mp4",
                    "fitness": "gym_background.mp4",
                    "traditional": "traditional_stage.mp4",
                    "unknown": "neutral_background.mp4"
                }
            },
            "quality": {
                "min_duration": 5,
                "max_duration": 600,
                "target_fps": 30,
                "target_resolution": [1920, 1080]
            }
        }
        
        config_file = Path(config_path)
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                self.deep_update(default_config, user_config)
        else:
            config_file.parent.mkdir(parents=True, exist_ok=True)
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            logger.info(f"åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {config_path}")
            
        return default_config
    
    def deep_update(self, base_dict, update_dict):
        """æ·±åº¦æ›´æ–°å­—å…¸"""
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self.deep_update(base_dict[key], value)
            else:
                base_dict[key] = value
    
    def setup_tools(self):
        """æ£€æŸ¥å’Œè®¾ç½®å·¥å…·"""
        # åˆ›å»ºå¿…è¦ç›®å½•
        for directory in [
            self.config["processing"]["temp_directory"],
            self.config["output"]["base_directory"],
            self.config["backgrounds"]["library_path"]
        ]:
            Path(directory).mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥GPU
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                logger.info(f"GPUæ£€æµ‹: {gpu.name} ({gpu.memoryTotal}MB)")
                if gpu.memoryTotal < 8000:  # å°äº8GB
                    logger.warning("âš ï¸ GPUå†…å­˜å¯èƒ½ä¸è¶³ï¼Œå»ºè®®é™ä½å¹¶å‘æ•°")
            else:
                logger.warning("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUå¤„ç†")
        except Exception as e:
            logger.warning(f"âš ï¸ GPUæ£€æµ‹å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥å·¥å…·å¯ç”¨æ€§
        tools_status = {}
        
        # æ£€æŸ¥Real-ESRGAN
        realesrgan_path = self.config["tools"]["realesrgan_path"]
        if Path(realesrgan_path).exists():
            tools_status["realesrgan"] = "available"
        else:
            tools_status["realesrgan"] = "missing"
            logger.warning(f"âš ï¸ Real-ESRGANæœªæ‰¾åˆ°: {realesrgan_path}")
        
        # æ£€æŸ¥FFmpeg
        try:
            subprocess.run([self.config["tools"]["ffmpeg_path"], "-version"], 
                         capture_output=True, check=True)
            tools_status["ffmpeg"] = "available"
        except (subprocess.CalledProcessError, FileNotFoundError):
            tools_status["ffmpeg"] = "missing"
            logger.warning("âš ï¸ FFmpegæœªæ‰¾åˆ°")
        
        self.tools_status = tools_status
        logger.info(f"å·¥å…·çŠ¶æ€: {tools_status}")
    
    def get_video_info(self, video_path: str) -> Dict:
        """è·å–è§†é¢‘ä¿¡æ¯"""
        try:
            cap = cv2.VideoCapture(video_path)
            
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)
            duration = frame_count / fps if fps > 0 else 0
            
            cap.release()
            
            file_size = os.path.getsize(video_path) / 1024 / 1024  # MB
            
            return {
                "resolution": (width, height),
                "fps": fps,
                "duration": duration,
                "size_mb": file_size
            }
        except Exception as e:
            logger.error(f"è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {str(e)}")
            return {}
    
    def select_background(self, category: str) -> str:
        """é€‰æ‹©èƒŒæ™¯è§†é¢‘"""
        bg_config = self.config["backgrounds"]
        bg_library = Path(bg_config["library_path"])
        
        # ä¼˜å…ˆä½¿ç”¨åˆ†ç±»å¯¹åº”çš„èƒŒæ™¯
        if category in bg_config["default_backgrounds"]:
            bg_filename = bg_config["default_backgrounds"][category]
            bg_path = bg_library / bg_filename
            
            if bg_path.exists():
                return str(bg_path)
        
        # ä½¿ç”¨é»˜è®¤èƒŒæ™¯
        default_bg = bg_library / bg_config["default_backgrounds"]["unknown"]
        if default_bg.exists():
            return str(default_bg)
        
        # ä½¿ç”¨åº“ä¸­çš„ç¬¬ä¸€ä¸ªèƒŒæ™¯
        bg_files = list(bg_library.glob("*.mp4"))
        if bg_files:
            return str(bg_files[0])
        
        logger.warning("âš ï¸ æœªæ‰¾åˆ°åˆé€‚çš„èƒŒæ™¯è§†é¢‘")
        return ""
    
    def process_superresolution(self, job: PipelineJob) -> bool:
        """è¶…åˆ†è¾¨ç‡å¤„ç†"""
        if self.tools_status.get("realesrgan") != "available":
            logger.error("Real-ESRGANä¸å¯ç”¨ï¼Œè·³è¿‡è¶…åˆ†è¾¨ç‡å¤„ç†")
            job.superres_path = job.input_path
            return True
        
        logger.info(f"ğŸ” è¶…åˆ†è¾¨ç‡å¤„ç†: {job.job_id}")
        
        temp_dir = Path(self.config["processing"]["temp_directory"]) / job.job_id
        temp_dir.mkdir(parents=True, exist_ok=True)
        
        output_path = temp_dir / f"{job.job_id}_superres.mp4"
        
        try:
            # Real-ESRGANå‘½ä»¤
            realesrgan_cmd = [
                self.config["tools"]["realesrgan_path"],
                "-i", job.input_path,
                "-o", str(output_path),
                "-s", str(self.config["processing"]["superres_scale"]),
                "-m", self.config["processing"]["superres_model"],
                "-f", "mp4"
            ]
            
            # æ‰§è¡Œè¶…åˆ†è¾¨ç‡
            result = subprocess.run(realesrgan_cmd, capture_output=True, text=True)
            
            if result.returncode == 0 and output_path.exists():
                job.superres_path = str(output_path)
                logger.info(f"âœ… è¶…åˆ†è¾¨ç‡å®Œæˆ: {output_path}")
                return True
            else:
                raise Exception(f"Real-ESRGANå¤±è´¥: {result.stderr}")
                
        except Exception as e:
            logger.error(f"âŒ è¶…åˆ†è¾¨ç‡å¤±è´¥: {str(e)}")
            job.superres_path = job.input_path  # ä½¿ç”¨åŸå§‹æ–‡ä»¶
            return False
    
    def process_matting(self, job: PipelineJob) -> bool:
        """è§†é¢‘æŠ åƒå¤„ç†"""
        logger.info(f"âœ‚ï¸ è§†é¢‘æŠ åƒ: {job.job_id}")
        
        temp_dir = Path(self.config["processing"]["temp_directory"]) / job.job_id
        input_path = job.superres_path or job.input_path
        
        # æå–å¸§
        frames_dir = temp_dir / "frames"
        frames_dir.mkdir(exist_ok=True)
        
        try:
            # ä½¿ç”¨FFmpegæå–å¸§
            extract_cmd = [
                self.config["tools"]["ffmpeg_path"],
                "-i", input_path,
                "-vf", "fps=30",  # å›ºå®š30fps
                str(frames_dir / "frame_%06d.png")
            ]
            
            subprocess.run(extract_cmd, check=True, capture_output=True)
            
            # è¿™é‡Œåº”è¯¥è°ƒç”¨RVMæˆ–å…¶ä»–æŠ åƒå·¥å…·
            # ç®€åŒ–ç‰ˆæœ¬ï¼šç›´æ¥å¤åˆ¶è¾“å…¥ï¼ˆå®é™…åº”ç”¨ä¸­éœ€è¦å®ç°çœŸå®çš„æŠ åƒï¼‰
            alpha_dir = temp_dir / "alpha"
            alpha_dir.mkdir(exist_ok=True)
            
            # æ¨¡æ‹ŸæŠ åƒè¿‡ç¨‹ï¼ˆå®é™…éœ€è¦RVMï¼‰
            frame_files = sorted(frames_dir.glob("*.png"))
            for i, frame_file in enumerate(frame_files):
                alpha_file = alpha_dir / f"alpha_{i:06d}.png"
                shutil.copy(frame_file, alpha_file)
            
            # é‡æ–°ç»„åˆä¸ºè§†é¢‘
            output_path = temp_dir / f"{job.job_id}_matted.mp4"
            
            compose_cmd = [
                self.config["tools"]["ffmpeg_path"],
                "-r", "30",
                "-i", str(alpha_dir / "alpha_%06d.png"),
                "-c:v", "libx264",
                "-pix_fmt", "yuv420p",
                str(output_path)
            ]
            
            subprocess.run(compose_cmd, check=True, capture_output=True)
            
            if output_path.exists():
                job.matting_path = str(output_path)
                logger.info(f"âœ… æŠ åƒå®Œæˆ: {output_path}")
                return True
            else:
                raise Exception("æŠ åƒè¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            logger.error(f"âŒ æŠ åƒå¤±è´¥: {str(e)}")
            job.matting_path = input_path  # ä½¿ç”¨å‰ä¸€æ­¥çš„ç»“æœ
            return False
    
    def process_background_replacement(self, job: PipelineJob) -> bool:
        """èƒŒæ™¯æ›¿æ¢å¤„ç†"""
        logger.info(f"ğŸ¨ èƒŒæ™¯æ›¿æ¢: {job.job_id}")
        
        temp_dir = Path(self.config["processing"]["temp_directory"]) / job.job_id
        input_path = job.matting_path or job.superres_path or job.input_path
        
        # é€‰æ‹©èƒŒæ™¯
        background_path = self.select_background(job.category)
        if not background_path:
            logger.warning("âš ï¸ æœªæ‰¾åˆ°èƒŒæ™¯ï¼Œè·³è¿‡èƒŒæ™¯æ›¿æ¢")
            job.background_path = input_path
            return True
        
        output_path = temp_dir / f"{job.job_id}_background.mp4"
        
        try:
            # è·å–è§†é¢‘ä¿¡æ¯
            video_info = self.get_video_info(input_path)
            target_res = self.config["quality"]["target_resolution"]
            
            # FFmpegèƒŒæ™¯åˆæˆå‘½ä»¤
            bg_cmd = [
                self.config["tools"]["ffmpeg_path"],
                "-i", background_path,
                "-i", input_path,
                "-filter_complex", 
                f"[0:v]scale={target_res[0]}:{target_res[1]}:force_original_aspect_ratio=increase,crop={target_res[0]}:{target_res[1]}[bg];"
                f"[1:v]scale={target_res[0]}:{target_res[1]}:force_original_aspect_ratio=decrease[fg];"
                f"[bg][fg]overlay=(W-w)/2:(H-h)/2",
                "-c:v", self.config["output"]["video_codec"],
                "-b:v", self.config["output"]["video_bitrate"],
                "-c:a", self.config["output"]["audio_codec"],
                "-shortest",  # ä»¥è¾ƒçŸ­çš„è§†é¢‘ä¸ºå‡†
                str(output_path)
            ]
            
            result = subprocess.run(bg_cmd, capture_output=True, text=True)
            
            if result.returncode == 0 and output_path.exists():
                job.background_path = str(output_path)
                logger.info(f"âœ… èƒŒæ™¯æ›¿æ¢å®Œæˆ: {output_path}")
                return True
            else:
                raise Exception(f"èƒŒæ™¯æ›¿æ¢å¤±è´¥: {result.stderr}")
                
        except Exception as e:
            logger.error(f"âŒ èƒŒæ™¯æ›¿æ¢å¤±è´¥: {str(e)}")
            job.background_path = input_path
            return False
    
    def finalize_output(self, job: PipelineJob) -> bool:
        """æœ€ç»ˆè¾“å‡ºå¤„ç†"""
        logger.info(f"ğŸ“¦ æœ€ç»ˆè¾“å‡º: {job.job_id}")
        
        input_path = job.background_path or job.matting_path or job.superres_path or job.input_path
        
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        base_dir = Path(self.config["output"]["base_directory"])
        if self.config["output"]["organize_by_category"]:
            output_dir = base_dir / job.category
        else:
            output_dir = base_dir
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        final_filename = f"{job.job_id}_final_{timestamp}.mp4"
        final_path = output_dir / final_filename
        
        try:
            # æœ€ç»ˆè´¨é‡ä¼˜åŒ–
            final_cmd = [
                self.config["tools"]["ffmpeg_path"],
                "-i", input_path,
                "-c:v", self.config["output"]["video_codec"],
                "-b:v", self.config["output"]["video_bitrate"],
                "-c:a", self.config["output"]["audio_codec"],
                "-movflags", "+faststart",  # ä¼˜åŒ–æµåª’ä½“æ’­æ”¾
                str(final_path)
            ]
            
            result = subprocess.run(final_cmd, capture_output=True, text=True)
            
            if result.returncode == 0 and final_path.exists():
                job.final_path = str(final_path)
                
                # è·å–è¾“å‡ºä¿¡æ¯
                output_info = self.get_video_info(str(final_path))
                job.output_resolution = output_info.get("resolution")
                job.output_size_mb = output_info.get("size_mb")
                
                logger.info(f"âœ… æœ€ç»ˆè¾“å‡ºå®Œæˆ: {final_path}")
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if not self.config["processing"]["keep_intermediates"]:
                    temp_dir = Path(self.config["processing"]["temp_directory"]) / job.job_id
                    if temp_dir.exists():
                        shutil.rmtree(temp_dir)
                        logger.info("ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶")
                
                return True
            else:
                raise Exception(f"æœ€ç»ˆè¾“å‡ºå¤±è´¥: {result.stderr}")
                
        except Exception as e:
            logger.error(f"âŒ æœ€ç»ˆè¾“å‡ºå¤±è´¥: {str(e)}")
            return False
    
    def process_single_job(self, job: PipelineJob) -> bool:
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        logger.info(f"ğŸ¬ å¼€å§‹GPU Pipeline: {job.job_id}")
        
        job.status = "processing"
        job.start_time = datetime.now().isoformat()
        start_time = datetime.now()
        
        # è·å–è¾“å…¥ä¿¡æ¯
        input_info = self.get_video_info(job.input_path)
        job.input_resolution = input_info.get("resolution")
        job.input_size_mb = input_info.get("size_mb")
        
        try:
            # å¤„ç†æµç¨‹
            stages = [
                ("superres", self.process_superresolution),
                ("matting", self.process_matting),
                ("background", self.process_background_replacement),
                ("finalize", self.finalize_output)
            ]
            
            for stage_name, stage_func in stages:
                job.stage = stage_name
                logger.info(f"ğŸ“ æ‰§è¡Œé˜¶æ®µ: {stage_name}")
                
                if not stage_func(job):
                    logger.warning(f"âš ï¸ é˜¶æ®µ {stage_name} å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œ")
            
            # æ£€æŸ¥æœ€ç»ˆç»“æœ
            if job.final_path and Path(job.final_path).exists():
                job.status = "completed"
                logger.info(f"âœ… Pipelineå®Œæˆ: {job.job_id}")
                return True
            else:
                raise Exception("æœ€ç»ˆè¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨")
                
        except Exception as e:
            job.status = "failed"
            job.error_message = str(e)
            logger.error(f"âŒ Pipelineå¤±è´¥: {job.job_id} - {str(e)}")
            return False
        
        finally:
            job.end_time = datetime.now().isoformat()
            job.processing_time = (datetime.now() - start_time).total_seconds()
            self.job_history.append(job)
    
    def load_viggle_results(self, viggle_results_dir: str = "./viggle_results") -> List[PipelineJob]:
        """åŠ è½½Viggleå¤„ç†ç»“æœ"""
        results_dir = Path(viggle_results_dir)
        if not results_dir.exists():
            logger.error(f"Viggleç»“æœç›®å½•ä¸å­˜åœ¨: {viggle_results_dir}")
            return []
        
        jobs = []
        
        # éå†ç»“æœæ–‡ä»¶
        for video_file in results_dir.rglob("*.mp4"):
            job_id = video_file.stem
            category = video_file.parent.name if video_file.parent != results_dir else "unknown"
            
            job = PipelineJob(
                input_path=str(video_file),
                job_id=job_id,
                category=category
            )
            jobs.append(job)
        
        logger.info(f"åŠ è½½äº† {len(jobs)} ä¸ªViggleç»“æœ")
        return jobs
    
    def run_batch_processing(self, input_directory: str = None):
        """è¿è¡Œæ‰¹é‡å¤„ç†"""
        logger.info("ğŸš€ GPU Pipelineæ¨¡å—å¯åŠ¨")
        
        # åŠ è½½ä»»åŠ¡
        if input_directory:
            jobs = self.load_viggle_results(input_directory)
        else:
            jobs = self.load_viggle_results()
        
        if not jobs:
            logger.warning("âš ï¸ æ²¡æœ‰å¾…å¤„ç†çš„ä»»åŠ¡")
            return
        
        # å¹¶è¡Œå¤„ç†
        max_workers = self.config["processing"]["max_concurrent_jobs"]
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            futures = [executor.submit(self.process_single_job, job) for job in jobs]
            
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                try:
                    result = future.result()
                    logger.info(f"ğŸ“Š è¿›åº¦: {i+1}/{len(jobs)} ({'âœ…' if result else 'âŒ'})")
                except Exception as e:
                    logger.error(f"âŒ ä»»åŠ¡å¼‚å¸¸: {str(e)}")
        
        # ç”ŸæˆæŠ¥å‘Š
        self.generate_pipeline_report()
    
    def generate_pipeline_report(self):
        """ç”ŸæˆPipelineæŠ¥å‘Š"""
        if not self.job_history:
            return
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_jobs = len(self.job_history)
        completed_jobs = len([j for j in self.job_history if j.status == "completed"])
        failed_jobs = len([j for j in self.job_history if j.status == "failed"])
        
        success_rate = (completed_jobs / total_jobs * 100) if total_jobs > 0 else 0
        
        # å¤„ç†æ—¶é—´ç»Ÿè®¡
        processing_times = [j.processing_time for j in self.job_history if j.processing_time]
        avg_processing_time = sum(processing_times) / len(processing_times) if processing_times else 0
        
        # åˆ†ç±»ç»Ÿè®¡
        category_stats = {}
        for job in self.job_history:
            cat = job.category
            if cat not in category_stats:
                category_stats[cat] = {"total": 0, "completed": 0, "failed": 0}
            category_stats[cat]["total"] += 1
            if job.status == "completed":
                category_stats[cat]["completed"] += 1
            elif job.status == "failed":
                category_stats[cat]["failed"] += 1
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_jobs": total_jobs,
                "completed_jobs": completed_jobs,
                "failed_jobs": failed_jobs,
                "success_rate": f"{success_rate:.1f}%",
                "average_processing_time": f"{avg_processing_time:.1f}ç§’"
            },
            "category_statistics": category_stats,
            "job_details": [
                {
                    "job_id": job.job_id,
                    "input_path": job.input_path,
                    "final_path": job.final_path,
                    "category": job.category,
                    "status": job.status,
                    "processing_time": job.processing_time,
                    "input_resolution": job.input_resolution,
                    "output_resolution": job.output_resolution,
                    "input_size_mb": job.input_size_mb,
                    "output_size_mb": job.output_size_mb,
                    "error_message": job.error_message
                }
                for job in self.job_history
            ]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"reports/gpu_pipeline_report_{timestamp}.json"
        Path(report_file).parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ¥ GPU Pipelineå¤„ç†æŠ¥å‘Š")
        print("="*60)
        print(f"ğŸ“Š æ€»è®¡ä»»åŠ¡: {total_jobs}")
        print(f"âœ… å®Œæˆ: {completed_jobs}")
        print(f"âŒ å¤±è´¥: {failed_jobs}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"â±ï¸ å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time:.1f}ç§’")
        print(f"ğŸ“‹ åˆ†ç±»ç»Ÿè®¡: {category_stats}")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        print("="*60)
        
        return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¥ Danceé¡¹ç›® - æœ¬åœ°GPU Pipelineæ¨¡å—")
    print("="*50)
    
    pipeline = LocalGPUPipeline()
    pipeline.run_batch_processing()

if __name__ == "__main__":
    main()
