#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Viggleè‡ªåŠ¨åŒ–æ¨¡å—
åŸºäºé¢„å¤„ç†é˜Ÿåˆ—ï¼Œä½¿ç”¨Playwrightè¿›è¡Œæ‰¹é‡è‡ªåŠ¨åŒ–å¤„ç†
"""

import asyncio
import json
import time
from pathlib import Path
from typing import List, Dict, Optional
from dataclasses import dataclass
import logging
from datetime import datetime

# å¤ç”¨ä¹‹å‰çš„ä¼˜åŒ–ç‰ˆPlaywrightä»£ç 
from scripts.viggle_playwright_optimized import ViggleProcessor, ViggleTask

logger = logging.getLogger(__name__)

@dataclass
class ViggleJob:
    """Viggleå¤„ç†ä»»åŠ¡"""
    video_path: str
    video_id: str
    priority: int
    estimated_credits: int
    category: str
    status: str = "pending"  # pending, processing, completed, failed
    result_path: Optional[str] = None
    error_message: Optional[str] = None
    start_time: Optional[str] = None
    end_time: Optional[str] = None
    actual_credits: Optional[int] = None

class ViggleAutomationModule:
    def __init__(self, config_path: str = "config/viggle_automation_config.json"):
        self.config = self.load_config(config_path)
        self.processor = ViggleProcessor()
        self.job_history = []
        
    def load_config(self, config_path: str) -> dict:
        """åŠ è½½è‡ªåŠ¨åŒ–é…ç½®"""
        default_config = {
            "queue_settings": {
                "input_queue_file": "./modules/processing_queue.json",
                "batch_size": 5,
                "max_concurrent_jobs": 2,
                "priority_threshold": 5
            },
            "viggle_settings": {
                "max_daily_credits": 100,
                "max_retries_per_video": 3,
                "timeout_minutes": 15,
                "cool_down_minutes": 2
            },
            "output_settings": {
                "result_directory": "./viggle_results",
                "organize_by_category": True,
                "keep_originals": True
            },
            "monitoring": {
                "enable_notifications": False,
                "webhook_url": "",
                "log_level": "INFO"
            }
        }
        
        config_file = Path(config_path)
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                user_config = json.load(f)
                self.deep_update(default_config, user_config)
        else:
            config_file.parent.mkdir(parents=True, exist_ok=True)
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, indent=2, ensure_ascii=False)
            logger.info(f"åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶: {config_path}")
            
        return default_config
    
    def deep_update(self, base_dict, update_dict):
        """æ·±åº¦æ›´æ–°å­—å…¸"""
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self.deep_update(base_dict[key], value)
            else:
                base_dict[key] = value
    
    def load_processing_queue(self) -> List[ViggleJob]:
        """åŠ è½½é¢„å¤„ç†é˜Ÿåˆ—"""
        queue_file = self.config["queue_settings"]["input_queue_file"]
        
        if not Path(queue_file).exists():
            logger.error(f"é˜Ÿåˆ—æ–‡ä»¶ä¸å­˜åœ¨: {queue_file}")
            return []
        
        try:
            with open(queue_file, 'r', encoding='utf-8') as f:
                queue_data = json.load(f)
            
            jobs = []
            for video_path in queue_data.get("queue", []):
                # ä»é¢„å¤„ç†æŠ¥å‘Šä¸­è·å–è¯¦ç»†ä¿¡æ¯
                video_id = Path(video_path).stem
                jobs.append(ViggleJob(
                    video_path=video_path,
                    video_id=video_id,
                    priority=7,  # é»˜è®¤ä¼˜å…ˆçº§ï¼Œå®é™…åº”ä»é¢„å¤„ç†ç»“æœè·å–
                    estimated_credits=2,  # é»˜è®¤ç§¯åˆ†ï¼Œå®é™…åº”ä»é¢„å¤„ç†ç»“æœè·å–
                    category="unknown"  # é»˜è®¤åˆ†ç±»ï¼Œå®é™…åº”ä»é¢„å¤„ç†ç»“æœè·å–
                ))
            
            logger.info(f"åŠ è½½äº† {len(jobs)} ä¸ªå¾…å¤„ç†ä»»åŠ¡")
            return jobs
            
        except Exception as e:
            logger.error(f"åŠ è½½é˜Ÿåˆ—å¤±è´¥: {str(e)}")
            return []
    
    def filter_jobs_by_credits(self, jobs: List[ViggleJob]) -> List[ViggleJob]:
        """æ ¹æ®ç§¯åˆ†é™åˆ¶ç­›é€‰ä»»åŠ¡"""
        max_credits = self.config["viggle_settings"]["max_daily_credits"]
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        sorted_jobs = sorted(jobs, key=lambda x: -x.priority)
        
        selected_jobs = []
        total_credits = 0
        
        for job in sorted_jobs:
            if total_credits + job.estimated_credits <= max_credits:
                selected_jobs.append(job)
                total_credits += job.estimated_credits
            else:
                logger.info(f"ç§¯åˆ†é™åˆ¶ï¼Œè·³è¿‡ä»»åŠ¡: {job.video_id}")
        
        logger.info(f"æŒ‰ç§¯åˆ†ç­›é€‰åå‰©ä½™ {len(selected_jobs)} ä¸ªä»»åŠ¡ï¼Œé¢„è®¡æ¶ˆè€— {total_credits} ç§¯åˆ†")
        return selected_jobs
    
    def organize_output_path(self, job: ViggleJob, result_filename: str) -> str:
        """ç»„ç»‡è¾“å‡ºè·¯å¾„"""
        base_dir = Path(self.config["output_settings"]["result_directory"])
        
        if self.config["output_settings"]["organize_by_category"]:
            output_dir = base_dir / job.category
        else:
            output_dir = base_dir
        
        output_dir.mkdir(parents=True, exist_ok=True)
        return str(output_dir / result_filename)
    
    async def process_single_job(self, job: ViggleJob) -> bool:
        """å¤„ç†å•ä¸ªä»»åŠ¡"""
        logger.info(f"ğŸ¬ å¼€å§‹å¤„ç†ä»»åŠ¡: {job.video_id}")
        
        job.status = "processing"
        job.start_time = datetime.now().isoformat()
        
        try:
            # åˆ›å»ºViggleä»»åŠ¡
            viggle_task = ViggleTask(
                src_path=job.video_path,
                task_id=job.video_id,
                account_id="primary"  # ç®€åŒ–ç‰ˆï¼Œå®é™…åº”æ”¯æŒå¤šè´¦å·
            )
            
            # å¤„ç†ä»»åŠ¡
            result_path = await self.processor.process_single_task(viggle_task)
            
            if result_path:
                # ç»„ç»‡è¾“å‡ºæ–‡ä»¶
                result_filename = Path(result_path).name
                organized_path = self.organize_output_path(job, result_filename)
                
                # ç§»åŠ¨æ–‡ä»¶åˆ°ç»„ç»‡åŒ–ç›®å½•
                Path(result_path).rename(organized_path)
                
                job.status = "completed"
                job.result_path = organized_path
                job.end_time = datetime.now().isoformat()
                
                logger.info(f"âœ… ä»»åŠ¡å®Œæˆ: {job.video_id} -> {organized_path}")
                return True
            else:
                raise Exception("å¤„ç†ç»“æœä¸ºç©º")
                
        except Exception as e:
            job.status = "failed"
            job.error_message = str(e)
            job.end_time = datetime.now().isoformat()
            
            logger.error(f"âŒ ä»»åŠ¡å¤±è´¥: {job.video_id} - {str(e)}")
            return False
        
        finally:
            self.job_history.append(job)
    
    async def run_batch_processing(self):
        """è¿è¡Œæ‰¹é‡å¤„ç†"""
        logger.info("ğŸš€ Viggleè‡ªåŠ¨åŒ–æ¨¡å—å¯åŠ¨")
        
        # åŠ è½½é˜Ÿåˆ—
        jobs = self.load_processing_queue()
        if not jobs:
            logger.warning("âš ï¸ æ²¡æœ‰å¾…å¤„ç†çš„ä»»åŠ¡")
            return
        
        # æ ¹æ®ç§¯åˆ†é™åˆ¶ç­›é€‰
        selected_jobs = self.filter_jobs_by_credits(jobs)
        if not selected_jobs:
            logger.warning("âš ï¸ ç§¯åˆ†é™åˆ¶ï¼Œæ²¡æœ‰å¯å¤„ç†çš„ä»»åŠ¡")
            return
        
        # æ‰¹é‡å¤„ç†
        batch_size = self.config["queue_settings"]["batch_size"]
        max_concurrent = self.config["queue_settings"]["max_concurrent_jobs"]
        
        for i in range(0, len(selected_jobs), batch_size):
            batch = selected_jobs[i:i+batch_size]
            logger.info(f"ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} ä¸ªä»»åŠ¡")
            
            # æ§åˆ¶å¹¶å‘
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_with_semaphore(job):
                async with semaphore:
                    return await self.process_single_job(job)
            
            # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
            results = await asyncio.gather(
                *[process_with_semaphore(job) for job in batch],
                return_exceptions=True
            )
            
            # ç»Ÿè®¡ç»“æœ
            success_count = sum(1 for r in results if r is True)
            logger.info(f"ğŸ“Š æ‰¹æ¬¡å®Œæˆ: {success_count}/{len(batch)} æˆåŠŸ")
            
            # æ‰¹æ¬¡é—´å†·å´
            if i + batch_size < len(selected_jobs):
                cool_down = self.config["viggle_settings"]["cool_down_minutes"] * 60
                logger.info(f"ğŸ˜´ æ‰¹æ¬¡é—´å†·å´ {cool_down/60:.1f} åˆ†é’Ÿ...")
                await asyncio.sleep(cool_down)
        
        # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
        self.generate_processing_report()
    
    def generate_processing_report(self):
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        if not self.job_history:
            return
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_jobs = len(self.job_history)
        completed_jobs = len([j for j in self.job_history if j.status == "completed"])
        failed_jobs = len([j for j in self.job_history if j.status == "failed"])
        
        success_rate = (completed_jobs / total_jobs * 100) if total_jobs > 0 else 0
        
        # ç§¯åˆ†ç»Ÿè®¡
        total_estimated_credits = sum(j.estimated_credits for j in self.job_history)
        
        # åˆ†ç±»ç»Ÿè®¡
        category_stats = {}
        for job in self.job_history:
            cat = job.category
            if cat not in category_stats:
                category_stats[cat] = {"total": 0, "completed": 0, "failed": 0}
            category_stats[cat]["total"] += 1
            if job.status == "completed":
                category_stats[cat]["completed"] += 1
            elif job.status == "failed":
                category_stats[cat]["failed"] += 1
        
        # ç”ŸæˆæŠ¥å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_jobs": total_jobs,
                "completed_jobs": completed_jobs,
                "failed_jobs": failed_jobs,
                "success_rate": f"{success_rate:.1f}%",
                "total_estimated_credits": total_estimated_credits
            },
            "category_statistics": category_stats,
            "job_details": [
                {
                    "video_id": job.video_id,
                    "video_path": job.video_path,
                    "category": job.category,
                    "priority": job.priority,
                    "status": job.status,
                    "result_path": job.result_path,
                    "error_message": job.error_message,
                    "start_time": job.start_time,
                    "end_time": job.end_time,
                    "estimated_credits": job.estimated_credits
                }
                for job in self.job_history
            ]
        }
        
        # ä¿å­˜æŠ¥å‘Š
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = f"reports/viggle_automation_report_{timestamp}.json"
        Path(report_file).parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æ‘˜è¦
        print("\n" + "="*60)
        print("ğŸ­ Viggleè‡ªåŠ¨åŒ–å¤„ç†æŠ¥å‘Š")
        print("="*60)
        print(f"ğŸ“Š æ€»è®¡ä»»åŠ¡: {total_jobs}")
        print(f"âœ… å®Œæˆ: {completed_jobs}")
        print(f"âŒ å¤±è´¥: {failed_jobs}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"ğŸ’° æ¶ˆè€—ç§¯åˆ†: {total_estimated_credits}")
        print(f"ğŸ“‹ åˆ†ç±»ç»Ÿè®¡: {category_stats}")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
        print("="*60)
        
        return report

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ Danceé¡¹ç›® - Viggleè‡ªåŠ¨åŒ–æ¨¡å—")
    print("="*50)
    
    automation = ViggleAutomationModule()
    await automation.run_batch_processing()

if __name__ == "__main__":
    asyncio.run(main())
