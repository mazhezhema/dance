#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å•è´¦å·å¤§æ‰¹é‡å¤„ç†å™¨
é’ˆå¯¹200ä¸ªè§†é¢‘ï¼Œ10ä¸ªäººç‰©ï¼Œ10ä¸ªèƒŒæ™¯çš„ä¼˜åŒ–é…ç½®
"""

import asyncio
import json
import time
import random
from pathlib import Path
from datetime import datetime, timedelta
import logging

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from src.core.task_database import task_db, TaskStatus
from src.automation.viggle_automation import ViggleEnhancedProcessor

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/batch_processor.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BatchProcessor:
    def __init__(self):
        self.config = self.load_config()
        self.processor = ViggleEnhancedProcessor()
        
    def load_config(self):
        """åŠ è½½é…ç½®"""
        config_file = Path("config/viggle_config.json")
        if config_file.exists():
            with open(config_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    async def estimate_processing_time(self, video_count: int) -> dict:
        """ä¼°ç®—å¤„ç†æ—¶é—´"""
        # å•è§†é¢‘å¤„ç†æ—¶é—´ä¼°ç®—
        avg_video_duration = 60  # å‡è®¾å¹³å‡60ç§’
        processing_time_per_video = avg_video_duration * 2  # å¤„ç†æ—¶é—´çº¦ä¸ºè§†é¢‘æ—¶é•¿çš„2å€
        rate_limit_avg = (self.config.get("processing", {}).get("rate_limit_min", 60) + 
                         self.config.get("processing", {}).get("rate_limit_max", 120)) / 2
        
        total_processing_time = video_count * processing_time_per_video
        total_wait_time = video_count * rate_limit_avg
        total_time = total_processing_time + total_wait_time
        
        # è½¬æ¢ä¸ºå°æ—¶
        hours = total_time / 3600
        
        return {
            "video_count": video_count,
            "avg_video_duration": avg_video_duration,
            "processing_time_per_video": processing_time_per_video,
            "rate_limit_avg": rate_limit_avg,
            "total_processing_time": total_processing_time,
            "total_wait_time": total_wait_time,
            "total_time_hours": hours,
            "estimated_completion": datetime.now() + timedelta(hours=hours)
        }
    
    async def check_system_requirements(self) -> bool:
        """æ£€æŸ¥ç³»ç»Ÿè¦æ±‚"""
        logger.info("ğŸ” æ£€æŸ¥ç³»ç»Ÿè¦æ±‚...")
        
        # æ£€æŸ¥è¾“å…¥ç›®å½•
        input_dir = Path("tasks_in")
        if not input_dir.exists():
            logger.error("âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: tasks_in/")
            return False
        
        video_files = list(input_dir.glob("*.mp4"))
        logger.info(f"ğŸ“ æ‰¾åˆ° {len(video_files)} ä¸ªè§†é¢‘æ–‡ä»¶")
        
        # æ£€æŸ¥äººç‰©å›¾ç‰‡
        people_dir = Path("input/people")
        if not people_dir.exists():
            logger.warning("âš ï¸ äººç‰©å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: input/people/")
        else:
            people_files = list(people_dir.glob("*.jpg")) + list(people_dir.glob("*.png"))
            logger.info(f"ğŸ‘¤ æ‰¾åˆ° {len(people_files)} ä¸ªäººç‰©å›¾ç‰‡")
        
        # æ£€æŸ¥èƒŒæ™¯å›¾ç‰‡
        bg_dir = Path("backgrounds")
        if not bg_dir.exists():
            logger.warning("âš ï¸ èƒŒæ™¯å›¾ç‰‡ç›®å½•ä¸å­˜åœ¨: backgrounds/")
        else:
            bg_files = list(bg_dir.glob("*.jpg")) + list(bg_dir.glob("*.png"))
            logger.info(f"ğŸ¨ æ‰¾åˆ° {len(bg_files)} ä¸ªèƒŒæ™¯å›¾ç‰‡")
        
        # æ£€æŸ¥è´¦å·é…ç½®
        accounts_file = Path("config/accounts.json")
        if not accounts_file.exists():
            logger.error("âŒ è´¦å·é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: config/accounts.json")
            return False
        
        with open(accounts_file, 'r', encoding='utf-8') as f:
            accounts_config = json.load(f)
        
        if not accounts_config.get("accounts"):
            logger.error("âŒ æœªé…ç½®è´¦å·ä¿¡æ¯")
            return False
        
        logger.info(f"âœ… ç³»ç»Ÿæ£€æŸ¥é€šè¿‡ï¼Œé…ç½®äº† {len(accounts_config['accounts'])} ä¸ªè´¦å·")
        return True
    
    async def create_batch_tasks(self, batch_size: int = 50) -> list:
        """åˆ›å»ºåˆ†æ‰¹ä»»åŠ¡"""
        logger.info(f"ğŸ“‹ åˆ›å»ºåˆ†æ‰¹ä»»åŠ¡ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
        
        input_dir = Path("tasks_in")
        video_files = sorted(list(input_dir.glob("*.mp4")))
        
        batches = []
        for i in range(0, len(video_files), batch_size):
            batch = video_files[i:i + batch_size]
            batches.append(batch)
            logger.info(f"æ‰¹æ¬¡ {len(batches)}: {len(batch)} ä¸ªè§†é¢‘")
        
        return batches
    
    async def process_batch(self, batch_files: list, batch_num: int) -> dict:
        """å¤„ç†å•ä¸ªæ‰¹æ¬¡"""
        logger.info(f"ğŸš€ å¼€å§‹å¤„ç†æ‰¹æ¬¡ {batch_num} ({len(batch_files)} ä¸ªè§†é¢‘)")
        
        start_time = time.time()
        results = {
            "batch_num": batch_num,
            "total_files": len(batch_files),
            "completed": 0,
            "failed": 0,
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "duration": 0
        }
        
        try:
            # è¿è¡ŒViggleå¤„ç†
            await self.processor.run_batch_processing()
            
            # ç»Ÿè®¡ç»“æœ
            completed_tasks = task_db.get_completed_tasks()
            failed_tasks = task_db.get_failed_tasks()
            
            results["completed"] = len(completed_tasks)
            results["failed"] = len(failed_tasks)
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch_num} å¤„ç†å¤±è´¥: {str(e)}")
            results["failed"] = len(batch_files)
        
        results["end_time"] = datetime.now().isoformat()
        results["duration"] = time.time() - start_time
        
        logger.info(f"âœ… æ‰¹æ¬¡ {batch_num} å®Œæˆ: {results['completed']} æˆåŠŸ, {results['failed']} å¤±è´¥")
        return results
    
    async def run_full_batch_processing(self):
        """è¿è¡Œå®Œæ•´æ‰¹é‡å¤„ç†"""
        logger.info("ğŸ¯ å•è´¦å·å¤§æ‰¹é‡å¤„ç†å™¨å¯åŠ¨")
        logger.info("=" * 60)
        
        # æ£€æŸ¥ç³»ç»Ÿè¦æ±‚
        if not await self.check_system_requirements():
            logger.error("âŒ ç³»ç»Ÿæ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
            return
        
        # ä¼°ç®—å¤„ç†æ—¶é—´
        video_count = len(list(Path("tasks_in").glob("*.mp4")))
        time_estimate = await self.estimate_processing_time(video_count)
        
        logger.info("â±ï¸ å¤„ç†æ—¶é—´ä¼°ç®—:")
        logger.info(f"   è§†é¢‘æ•°é‡: {time_estimate['video_count']}")
        logger.info(f"   é¢„è®¡æ€»æ—¶é—´: {time_estimate['total_time_hours']:.1f} å°æ—¶")
        logger.info(f"   é¢„è®¡å®Œæˆæ—¶é—´: {time_estimate['estimated_completion']}")
        
        # ç¡®è®¤å¼€å§‹å¤„ç†
        confirm = input("\næ˜¯å¦å¼€å§‹å¤„ç†ï¼Ÿ(y/N): ").strip().lower()
        if confirm != 'y':
            logger.info("âŒ ç”¨æˆ·å–æ¶ˆå¤„ç†")
            return
        
        # åˆ›å»ºåˆ†æ‰¹ä»»åŠ¡
        batch_size = self.config.get("batch_processing", {}).get("batch_size", 50)
        batches = await self.create_batch_tasks(batch_size)
        
        # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
        all_results = []
        for i, batch in enumerate(batches, 1):
            logger.info(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i}/{len(batches)}")
            
            # å¤„ç†æ‰¹æ¬¡
            result = await self.process_batch(batch, i)
            all_results.append(result)
            
            # æ‰¹æ¬¡é—´æš‚åœ
            if i < len(batches):
                pause_time = self.config.get("batch_processing", {}).get("pause_between_batches", 1800)
                logger.info(f"ğŸ˜´ æ‰¹æ¬¡é—´æš‚åœ {pause_time} ç§’...")
                await asyncio.sleep(pause_time)
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        await self.print_final_statistics(all_results)
    
    async def print_final_statistics(self, results: list):
        """è¾“å‡ºæœ€ç»ˆç»Ÿè®¡"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“Š æ‰¹é‡å¤„ç†å®Œæˆç»Ÿè®¡")
        logger.info("=" * 60)
        
        total_files = sum(r["total_files"] for r in results)
        total_completed = sum(r["completed"] for r in results)
        total_failed = sum(r["failed"] for r in results)
        total_duration = sum(r["duration"] for r in results)
        
        logger.info(f"ğŸ“ æ€»æ–‡ä»¶æ•°: {total_files}")
        logger.info(f"âœ… æˆåŠŸå¤„ç†: {total_completed}")
        logger.info(f"âŒ å¤„ç†å¤±è´¥: {total_failed}")
        logger.info(f"ğŸ“ˆ æˆåŠŸç‡: {(total_completed/total_files*100):.1f}%")
        logger.info(f"â±ï¸ æ€»è€—æ—¶: {total_duration/3600:.1f} å°æ—¶")
        
        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                "total_files": total_files,
                "total_completed": total_completed,
                "total_failed": total_failed,
                "success_rate": total_completed/total_files*100,
                "total_duration_hours": total_duration/3600
            },
            "batch_results": results
        }
        
        report_file = Path("logs/batch_processing_report.json")
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_file}")

async def main():
    """ä¸»å‡½æ•°"""
    processor = BatchProcessor()
    await processor.run_full_batch_processing()

if __name__ == "__main__":
    asyncio.run(main())
